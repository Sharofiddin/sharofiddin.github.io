<!DOCTYPE html>
<html>
<head>
<link rel="Stylesheet" type="text/css" href="style.css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" />
<title>DesigningDataIntensiveApplications</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>

<div id="Overall"><h1 id="Overall" class="header"><a href="#Overall">Overall</a></h1></div>
<p>
Book focuses on these key concerns:
</p>
<ul>
<li>
Reliability

<li>
Scalability

<li>
Maintainability

</ul>
<p>
Data models:
</p>
<ul>
<li>
Relational

<li>
Document

<li>
Graph

</ul>
  
<p>
Storage engines:
</p>
<ul>
<li>
Log structured

<li>
Page oriented

</ul>
 
<p>
 Logs in general - append-only sequence of records.
 Pros:
</p>
<ul>
<li>
Fast read(with in-memory hashmap index)/write

<li>
Recovery. No update - no possibility of mixture of old data and new data in one record.

<li>
No fragmented files if merge used.

</ul>
<p>
Cons:
</p>
<ul>
<li>
Very large number of keys causes huge RAM consumption.

<li>
Range queries are not effective. If values for keys between <code>key10</code> and <code>key99</code> are requested for each key value will be fetched.

</ul>
<div id="Overall-Indexes"><h2 id="Indexes" class="header"><a href="#Overall-Indexes">Indexes</a></h2></div>
<p>
Index types:
</p>
<ul>
<li>
LSMTree(Log Structured Merge Tree)

<li>
B-Tree(Balanced Tree)

</ul>
<p>
B-Tree:
</p>
<ul>
<li>
B-Tree uses pages (or blocks) to store keys, references and values.

<li>
WAL (Write-Ahead-Log) used for crash recovery.

<li>
Reference count in one page named <code>branching factor</code>

<li>
Abbrevating keys saves space and causes high *<span id="Overall-Indexes-branching factor"></span><strong id="branching factor">branching factor</strong>*, so fewer levels, and this makes search faster.

</ul>
<p>
<code>Write amplification</code> - one write to database causes multiple writes.
</p>

<p>
LSMTree vs B-Tree:
</p>
<ul>
<li>
LSMTree faster write, it has lower <code>write amplification</code> than B-Tree. When B-Tree page fill it will be splitted to two new pages, and parent page keeps references to new children. This causes higher write amplification.

</ul>
<p>
...
</p>

<p>
Index has key and value. Key is the thing queries search for. For example ID, or name (if indexed). But value can be :
</p>
<ol>
<li>
actual row(document in mongodb, vertex in neo4j)

<li>
reference to the row(document, vertex) stored elsewhere.

</ol>
<p>
In second case, place that rows are stored is known as a <code>heap file</code>. Heap file is good to keep changes in one place. But when referenced heap location changed all indexes should be updated to point new location.
If indexed row directly stored with row (first case) it named <code>clustered index</code>
Other indexing structures:
</p>
<ul>
<li>
multi-column indexes (R-Tree).
  ...

</ul>
<p>
NVM - Non-volatile memory.
</p>

<div id="Replication"><h1 id="Replication" class="header"><a href="#Replication">Replication</a></h1></div>

<p>
Why we replicate data:
</p>
<ul>
<li>
To keep data geographically close to users.

<li>
To allow system to operate when some of its parts failed.

<li>
To scale out the number of machines that can serve to read queries (increase throughput)
 <span id="Replication-Popular replicating algorithms:"></span><strong id="Popular replicating algorithms:">Popular replicating algorithms:</strong>

<li>
single-leader

<li>
multi-leader

<li>
leaderless

</ul>
<div id="Replication-Leader based replication"><h2 id="Leader based replication" class="header"><a href="#Replication-Leader based replication">Leader based replication</a></h2></div>

<p>
When we replicate database, every write should processed by every replica to avoid data inconsistency between replicas. <span id="Replication-Leader based replication-Leader based replication"></span><strong id="Leader based replication">Leader based replication</strong> or <em>master-slave replication</em> is the most common solution for it.
It works as follows:
</p>
<ol>
<li>
One of the replicas is designated as <em>leader</em> (master/primary). Clients sends write requests to the leader. Leader first writes the new data to its local storage.

<li>
The other replicas are knows as <em>followers (read replicas, slaves, secondaries, or hot standbys)</em>. Whenever the leader writes new data to its local storage, it also sends the data change to all of its followers as part of a <em>replication log</em> or <em>change stream</em>. Each follower takes the log from the leader and updates it local copy of the DB accordingly, by applying all writes in the same order as they were processed on the leader

<li>
When client initiates read request it can be served by on of the standbys or leader. But, writes are only accepted on the leader. The followers are read-only from the client's point of view.

</ol>
<div id="Replication-Synchronous vs Asynchronous replication"><h2 id="Synchronous vs Asynchronous replication" class="header"><a href="#Replication-Synchronous vs Asynchronous replication">Synchronous vs Asynchronous replication</a></h2></div>

<p>
  Whether replication happens synchronously or asynchronously is important detail of replicated system. In relational database it is usually configurable option; in other systems it is often hardcoded.
Let's say we have one <span id="Replication-Synchronous vs Asynchronous replication-leader"></span><strong id="leader">leader</strong> and two <span id="Replication-Synchronous vs Asynchronous replication-replicas"></span><strong id="replicas">replicas</strong> for user management system. At some point in time, client requests <span id="Replication-Synchronous vs Asynchronous replication-the leader"></span><strong id="the leader">the leader</strong> to update some user's profile image. Request recieved by the leaeder after short time it was sent. At some point, the leader forwards data change to followers. Eventually, the leader notifies the client that the update was successfull. What happens if the leader sends update to the first replica synchronously and to the second asynchronously? The leader waits the first replica's confirmation about recieving the update successfully before notifying to the client, but no second's. 
  Usually, replication is fast (less than a second). But there is no guarante of how long it takes. It take several minutes in some cases: when replica is recovering from failuer, network problems between nodes, OS near maximum capacity. In these cases, asynchronously replicated follower may fall behind the leader by several minutes. On the otherhand, the synchronously replicated follower will block the leader from finishing the operation.
  Synchronous replication:
</p>
<ul>
<li>
Advantages:

<ul>
<li>
Up to date data with the Leader. Even, if the leader fails, all successfully written data is still available on the follower.

</ul>
<li>
Disadvantages:

<ul>
<li>
If the follower does not respond (crashed, network error,...), the write can not be processed. The leader must block all writes until the synchronous is available again.
  For this shortcoming, it is impractical to make all followers synchronous: If one of them fallen, the whole system stops operating. In practice: One of the followers is made synchronous. If synchronous followers fails or slows down, one of the asynchronous followers is made synchronous and we alwasy have two nodes with <em>up to date</em> data: The leader and the synchronous follower. This configuration sometimes called <em>semi-synchronous</em>.

</ul>
</ul>
  
<div id="Replication-Adding new followers"><h2 id="Adding new followers" class="header"><a href="#Replication-Adding new followers">Adding new followers</a></h2></div>

<p>
  Adding new followers to the cluster is not always straight forward task. We can not just copy and paste leader's data into new replica server. Because during data transfer process the leader my accept new writes. Or, the leader should be locked until new replica gets all data. But we want to do it without downtime:
</p>
<ol>
<li>
Take a consistent snapshot of the leader's database at some point in time-, if possible, without taking a lock on entire database. Most databases have thi feature, as it also required for backups.

<li>
Copy the snapshot to the new follower node.

<li>
The follower connects to the leader and requests all the data changes since snapshot was taken.

<li>
When the follower has processed the backlog of data changes since snapshot, it is called <em>caught up</em>. Now, it can continue to process data changes from the leader as they happen.

</ol>
<p>
...
</p>

<div id="Replication-Problems with replication lag"><h2 id="Problems with replication lag" class="header"><a href="#Replication-Problems with replication lag">Problems with replication lag</a></h2></div>

<ul>
<li>
<em>read-after-write consistency</em> == <em>read-your-writes consistency</em> - it ensures that if user reloads the page, they will always see any updates they submitted themselves, no promise about other user's update.

<li>
<em>monotonic reads</em> - let's say we have one leader and two replicas, user_1 added new comment, user_2 read this comment from replica_1, but when he refreshed page comment disappeared because this time user_2 read data from replica_2 which has not updated data yet. Monotonic reads guarantees that there is nothis kind of situation, by routing each user to exact datacenter whenever he reads data.

<li>
<em>consistent prefix reads</em> ...

</ul>
<div id="Replication-Multi-Leader replication"><h2 id="Multi-Leader replication" class="header"><a href="#Replication-Multi-Leader replication">Multi-Leader replication</a></h2></div>

<p>
  In single-leader replication if app can not connect to the leader it can not write to the database. If we have more than one leader connection problem with one leader won't stop us from writing to the database. It is called a <em>multi-leader</em> configuration. Also known as master-master or active/active replication. Each leader simultaneously acts as a follower to the other leaders.
</p>

<div id="Replication-Multi-Leader replication-Use cases for multi-leader replication"><h3 id="Use cases for multi-leader replication" class="header"><a href="#Replication-Multi-Leader replication-Use cases for multi-leader replication">Use cases for multi-leader replication</a></h3></div>

<div id="Replication-Multi-Leader replication-Use cases for multi-leader replication-Multi-datacenter operation"><h4 id="Multi-datacenter operation" class="header"><a href="#Replication-Multi-Leader replication-Use cases for multi-leader replication-Multi-datacenter operation">Multi-datacenter operation</a></h4></div>
  
<p>
  When we have single-leader setup with multiple datacenter, all writes go to the single leader, so any issue with leader's datacenter will stop all writes eventhough other datacenters operating normally, and synchronous replication can be serious bottleneck if sync-replica is in different datacenter from leader's. These issues of single-leader configuration dimnish advantages of multi-dataceneter setup.
  Advantages of multi-leader:
</p>
<ul>
<li>
Performance

<li>
Tolerance of datacenter outage

<li>
Tolerance of network problems
  Big Downside:

<li>
Concurrent update of data requires "conflict resolution".

</ul>
<div id="Replication-Multi-Leader replication-Use cases for multi-leader replication-Clients with offline operation"><h4 id="Clients with offline operation" class="header"><a href="#Replication-Multi-Leader replication-Use cases for multi-leader replication-Clients with offline operation">Clients with offline operation</a></h4></div>

<p>
  When application should work even it is disconnected from internet, multi-leader replication can be useful. Like calendar apps. Even your phone is offline you can see calendar and can mark some days for some events, when your gained access to the internet again your calendar reconsile its data with server. In this extreme case each device with your account is leader. 
</p>

<div id="Replication-Multi-Leader replication-Use cases for multi-leader replication-Collaborative editing"><h4 id="Collaborative editing" class="header"><a href="#Replication-Multi-Leader replication-Use cases for multi-leader replication-Collaborative editing">Collaborative editing</a></h4></div>

<p>
  <em>Real time collaborative editing</em> apps allow several people to edit a document simultaneously like Google Docs.
</p>
  
<div id="Replication-Multi-Leader replication-Handling Write Conflicts"><h3 id="Handling Write Conflicts" class="header"><a href="#Replication-Multi-Leader replication-Handling Write Conflicts">Handling Write Conflicts</a></h3></div>

<p>
...
</p>

<div id="Replication-Multi-Leader replication-Multi-Leader Replication Topologies"><h3 id="Multi-Leader Replication Topologies" class="header"><a href="#Replication-Multi-Leader replication-Multi-Leader Replication Topologies">Multi-Leader Replication Topologies</a></h3></div>

<p>
If Multi-Leader replication only has two leaders, it is trivial topology: both of them send writes to another and receive it. But when we have more than two Leaders, we have different topologies:
</p>
<ul>
<li>
circle

<li>
star

<li>
all-to-all

</ul>
<p>
In circle, each Leader nor recieves writes from one Leader node and sends writes (it's own and received from prior Leader) to other Leader.
In start topology, there is one Leader node which forwards all writes from other Leaders. It can be generalized to Tree topology.
All-to-all - each Leader sends writes to all other Leader nodes.
</p>

<p>
Cirlce and Star topologies are more prone to Single-Point-Of-failure.
All-to-all is fault tolerant but in ordering is complex problem in this topology.
</p>

<div id="Replication-Multi-Leader replication-Leaderless replication"><h3 id="Leaderless replication" class="header"><a href="#Replication-Multi-Leader replication-Leaderless replication">Leaderless replication</a></h3></div>

<p>
  <em>Clients send each write to several nodes, and read from several nodes in parallel in order to detect and correct nodes with stale data.</em>
</p>
  
<div id="Replication-Multi-Leader replication-Leaderless replication-Writing to the Database When a Node Is Down"><h4 id="Writing to the Database When a Node Is Down" class="header"><a href="#Replication-Multi-Leader replication-Leaderless replication-Writing to the Database When a Node Is Down">Writing to the Database When a Node Is Down</a></h4></div>
<p>
  <em>n</em> - Replication Factor: In leaderless replication the replication factor n is the number of nodes to store a replica of particular pieco of data. Total number of nodes in cluster can be greater from <em>n</em>. In this case, read and write nodes may or may not be overlapped. 
</p>
<div id="Replication-Multi-Leader replication-Leaderless replication-Limitations of Quorum Consistency"><h4 id="Limitations of Quorum Consistency" class="header"><a href="#Replication-Multi-Leader replication-Leaderless replication-Limitations of Quorum Consistency">Limitations of Quorum Consistency</a></h4></div>
<p>
 <em>Anti-entropy</em> - background process that compares data between nodes and detects and fixes inconsistencies. It is proactive process which periodically runs. In contrast, <em>read-repair</em> is reactive process only checks inconsistencies when data accessed for read. 
</p>

<div id="Replication-Multi-Leader replication-Leaderless replication-Sloppy Quorums and Hinted Handoff"><h4 id="Sloppy Quorums and Hinted Handoff" class="header"><a href="#Replication-Multi-Leader replication-Leaderless replication-Sloppy Quorums and Hinted Handoff">Sloppy Quorums and Hinted Handoff</a></h4></div>

<div id="Replication-Multi-Leader replication-Leaderless replication-Detecting Concurrent Writes"><h4 id="Detecting Concurrent Writes" class="header"><a href="#Replication-Multi-Leader replication-Leaderless replication-Detecting Concurrent Writes">Detecting Concurrent Writes</a></h4></div>

<p>
  In leaderless replication, recognizing what is concurrent write and what is overwrite is one of the important problems. Usually, versions are used for this task. In short, if client submits previous version in write than it is overwrite for older version, else it is concurrent writing. But, with more replicas the situation is not that simple.
 ...
</p>

<p>
Note: Amazon Dynamo systme is not DynamoDB. Yes, I was also confused.
</p>


<div id="Replication-6. Partitioning"><h2 id="6. Partitioning" class="header"><a href="#Replication-6. Partitioning">6. Partitioning</a></h2></div>
<p>
Partitioning or Sharding is breaking apart data into partitions.
</p>

<div id="Replication-6. Partitioning-Partitioning and Replication"><h3 id="Partitioning and Replication" class="header"><a href="#Replication-6. Partitioning-Partitioning and Replication">Partitioning and Replication</a></h3></div>

<p>
  Partitioning is counterpart of Replication. One node can contain both Leader and Follower partitions.
</p>
  
<div id="Replication-6. Partitioning-Partitioning of Key-Value Data"><h3 id="Partitioning of Key-Value Data" class="header"><a href="#Replication-6. Partitioning-Partitioning of Key-Value Data">Partitioning of Key-Value Data</a></h3></div>

<div id="Replication-6. Partitioning-Partitioning of Key-Value Data-Partitioning by Key Range"><h4 id="Partitioning by Key Range" class="header"><a href="#Replication-6. Partitioning-Partitioning of Key-Value Data-Partitioning by Key Range">Partitioning by Key Range</a></h4></div>
<p>
  Unfair partitioning - e.g. some partitions have more data or queries than others, also called <em>skewed</em>.
...
</p>
<div id="Replication-6. Partitioning-Partitioning of Key-Value Data-Partitioning by Hash of Key"><h4 id="Partitioning by Hash of Key" class="header"><a href="#Replication-6. Partitioning-Partitioning of Key-Value Data-Partitioning by Hash of Key">Partitioning by Hash of Key</a></h4></div>
<p>
  Java's <code>Object.hashCode()</code> is not suitable for distrubuted systems, as same object can have different hash values when object is complex object. For this reason, sharded databases should use more reliable hashing algorithms, not necessarily cryptographically strong. For example: Cassandra uses Murmur3. 
</p>

<p>
==== Skewed Workloads and Relieving Hot Spots
  Hashing wont solve skewing workload problems fully. In social networks celebrities have a lot of subscribers so a lot of posts/replies/comments. So, as each user as its own ID and hash for this ID everytime produces some output. It will cause to skewed workloads again in nodes where data of these celebritiesproblems fully. In social networks celebrities have a lot of subscribers so a lot of posts/replies/comments. So, as each user as its own ID and hash for this ID everytime produces some output. It will cause to skewed workloads stored. One workaroun: adding random one two digits after user_id (or whatever partitioning key), but this leads to query every partition for these records in read, and also some bookkeeping required: we shouldn't do this trick for ordinary users, this also adds some overhead.
</p>

<div id="Replication-6. Partitioning-Partitioning and Secondary Indexes"><h3 id="Partitioning and Secondary Indexes" class="header"><a href="#Replication-6. Partitioning-Partitioning and Secondary Indexes">Partitioning and Secondary Indexes</a></h3></div>

<p>
  Primary indexes are not always sufficient, let's suppose we have actions table with primary key action_id, and we also have <code>user_id</code> column which we want to add index for it. Secondary index column need not to be unique. Not all partitioned datastores adopted <em>secondary indexes</em> (HBase, Voldemort). But Riak KV added it to its features. And for search servers like Solr and Elasticsearch <em>secondary indexes</em> are must have. 
  Secondary index don't map neatly with partitions. There two approaches for partitioning databases with secondary indexes:
</p>
<ul>
<li>
document based partitioning

<li>
term-based partitioning

</ul>
<div id="Replication-6. Partitioning-Partitioning and Secondary Indexes-Partitioning secondary Indexes by Document"><h4 id="Partitioning secondary Indexes by Document" class="header"><a href="#Replication-6. Partitioning-Partitioning and Secondary Indexes-Partitioning secondary Indexes by Document">Partitioning secondary Indexes by Document</a></h4></div>

<p>
  In this approach secondary indexes are local, it means each partition has its own secondary indexes and it doesn't care about other partitions. One downside of this approach: when reading we should check every partition because this approach won't guarante that records with one secondary index (for example "color:red" ) stored in one partition or exact some partitions. 
</p>
<pre><code>This approach to querying a partitioned database is sometimes known as scatter/gather, and it can make read queries on secondary index quite expensive.
</pre></code>
<div id="Replication-6. Partitioning-Partitioning and Secondary Indexes-Partitioning Secondary Indexes by Term"><h4 id="Partitioning Secondary Indexes by Term" class="header"><a href="#Replication-6. Partitioning-Partitioning and Secondary Indexes-Partitioning Secondary Indexes by Term">Partitioning Secondary Indexes by Term</a></h4></div>

<p>
  In this approach we have a global index that covers data in all partitions. Seconday index itself also should be partitioned. Read is faster than scatter/gather but writes are slover and more complex, requires distributed transactions.
</p>

<div id="Replication-6. Partitioning-Rebalancing partitions"><h3 id="Rebalancing partitions" class="header"><a href="#Replication-6. Partitioning-Rebalancing partitions">Rebalancing partitions</a></h3></div>

<p>
  Sometimes things change in database:
</p>
<ul>
<li>
We need more CPU for processing queries

<li>
We should handle huge datasets so we need more RAM

<li>
Some nodes failed so we should add new nodes for their responsibilities.

</ul>
  
<p>
  In these cases we should move data and requests from one node to another, and this called <em>rebalancing</em>. Rebalancing minimum requirements:
</p>
<ul>
<li>
After rebalancing, the load should be shared fairly between the nodes in the cluster.

<li>
While rebalancing is happening, the database continue accepting reads and writes.

<li>
No more data than necessary should be moved between nodes.

</ul>
<div id="Replication-6. Partitioning-Rebalancing partitions-Strategies for Rebalancing"><h4 id="Strategies for Rebalancing" class="header"><a href="#Replication-6. Partitioning-Rebalancing partitions-Strategies for Rebalancing">Strategies for Rebalancing</a></h4></div>

<p>
...
</p>

<p>
=== Operations: Automatic or Manual Rebalancing
</p>
  
<p>
  Full automatic rebalancing mixed with automatic failure detection can be very dangerous. Because if some nodes are overloaded other nodes conclude overloaded node as dead node, and automatic rebalancing process starts to move load away from it. This puts additional load on the overloaded node, other nodes and network - resulting cascading failure which is worse.
</p>

<div id="Replication-6. Partitioning-Request Routing"><h3 id="Request Routing" class="header"><a href="#Replication-6. Partitioning-Request Routing">Request Routing</a></h3></div>

<p>
  ...
</p>

<div id="Replication-6. Partitioning-Request Routing-Parallel Query execution"><h4 id="Parallel Query execution" class="header"><a href="#Replication-6. Partitioning-Request Routing-Parallel Query execution">Parallel Query execution</a></h4></div>

<p>
  ...
</p>

<div id="Replication-6. Partitioning-Summary"><h3 id="Summary" class="header"><a href="#Replication-6. Partitioning-Summary">Summary</a></h3></div>

<p>
  ...
</p>

<div id="Replication-7. Transactions"><h2 id="7. Transactions" class="header"><a href="#Replication-7. Transactions">7. Transactions</a></h2></div>


<p>
<a href="index.html">index</a> -- index
</p>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script type="text/javascript">
    document.querySelectorAll('pre').forEach(block => hljs.highlightBlock(block));
</script>
</body>
</html>
